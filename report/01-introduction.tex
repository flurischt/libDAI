
\section{Introduction}\label{sec:intro}
%<Introduction of the Introduction>
% NJU:
% I'd emphasise the Belief Propagation, because
% 1) it was the initial motivation to work with this topic
% 2) most of our improvements are applicable to general BP and not just our example

Whether you are planing to segment an image \cite{1544822}, recognize speech \cite{5373446} or analyze medical datasets \cite{bailly2011finding}, \textit{Belief Propagation} (BP) plays an important role in all these research areas. This is due to the very general nature of the BP algorithm which allows to approximate the probabilities for certain events (e.g. a pixel belonging to an object, an audio fragment belonging to a language,...). In recent years, BP has also been applied successfully in the domain of recommender systems \cite{Ha:2012:TRT:2396761.2398636}. Popularized by the Netflix challenge and E-Commerce shops such as Amazon, recommender systems try to predict which movies / items a user wants to watch / buy based on his previous ratings as well as the ratings of other, often similar, users. Belief propagation can be used to compute the probability of a user to like a particular item. Given these probabilities we are able to rank items and output the Top-N items for the given user.

 
%The work at hand presents a run-time optimized implementation of loopy Belief Propagation, tuned for a Recommender System. In the following, we quickly motivate our work and present the structure of this document.


%Do not start the introduction with the abstract or a slightly modified
%version. It follows a possible structure of the introduction. 
%Note that the structure can be modified, but the
%content should be the same. Introduction and abstract should fill at most the first page, better less.

% Elias: We should keep that in mind because the introduction to the introduction sounds quite similiar to an abstract.

\mypar{Motivation} 
While the results of Jiwoon Hi et al.\cite{Ha:2012:TRT:2396761.2398636} looked very promising, one problem was that BP often takes a long time to converge. Even worse, because we needed to run BP once for every user (to predict his ratings), the system seemed to be impossible to use for a large number of users. In our initial tests a baseline implementation took about 21 hours to predict all the ratings on a fairly small dataset (80367 ratings from 943 users for 1683 movies). To change this a run-time efficient implementation of the BP algorithm was needed.
This would not only make this one particular approach feasible but also enable a wide range of other applications to benefit from the increased speed.

\mypar{Related work} 
We know two popular open-source C++ frameworks for graph-based inference that implement the message passing algorithm\footnote{This term will be used synonymously for BP in this article.} in a generic way: \textit{libDAI} \cite{Mooij_libDAI_10} on the one hand is a clean and accessible piece of software, whereas \textit{OpenGM} \cite{andres2012opengm} on the other hand is richer in feature and written with a higher degree of abstraction. We chose libDAI as our reference, mostly because it offers, as opposed to OpenGM, a native implementation of \textit{residual} BP (see below for an explanation). 

In the following, we briefly sketch the \textit{Top-N Recommender System based on Residual Belief Propagation} in section \ref{sec:background} before we extensively inform the reader in section \ref{sec:method} about the optimization techniques that have been applied. The results are presented in section \ref{sec:results}.