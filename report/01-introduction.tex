
\section{Introduction}\label{sec:intro}
%<Introduction of the Introduction>
% NJU:
% I'd emphasise the Belief Propagation, because
% 1) it was the initial motivation to work with this topic
% 2) most of our improvements are applicable to general BP and not just our example

Whether you are planing to segment an image \cite{1544822}, recognize speech \cite{5373446} or analyse medical datasets \cite{bailly2011finding}, belief propagation (BP) plays an important role in all these research areas. This is due to the very general nature of the BP algorithm which allows the user to calculate the probabilities for specific events (a pixel belonging to a region, an audio segment belonging to a language,...). In recent years an other use of BP has been discovered concerning the recommendation of items. Popularized by the Netflix challenge, recommendation systems try to predict which items a user wants to see based on his own ratings and the ratings of other users.

%The work at hand presents a run-time optimized implementation of loopy Belief Propagation, tuned for a Recommender System. In the following, we quickly motivate our work and present the structure of this document.


%Do not start the introduction with the abstract or a slightly modified
%version. It follows a possible structure of the introduction. 
%Note that the structure can be modified, but the
%content should be the same. Introduction and abstract should fill at most the first page, better less.

% Elias: We should keep that in mind because the introduction to the introduction sounds quite similiar to an abstract.

\mypar{Motivation} 
\textit{Belief Propagation} (BP) is a very general and therefore popular algorithm in the domain of machine learning. It has found application in many different areas, ranging from natural to social sciences. 
% Elias: To unspecifc, shouldn't we just include some examples here?
BP is exerted to do probabilistic inference on \textit{graphical models}, out of which Bayesian Networks and Markov Random Fields are the most well known.

The standard version of BP is designed to deal with acyclic graphs. The algorithm can, however, also be applied to graphical models containing loops, although convergence is no longer guaranteed. In this context BP is often called \textit{Loopy Belief Propagation} (LBP). Typical LBP problems are known for bad convergence and high computational costs. Nevertheless the approximated solution is often accurate enough to be used in real world applications.

In this project, LBP is used to build a \textit{Recommender System} (RS). In this system the goal is to generate a list of recommended items for a specific user of a platform or marketplace. The recommendation is based upon the ratings of other users and previous ratings of the user under consideration. With the huge success of online platforms such as Amazon, Youtube and Netflix, Recommender Systems gained a lot of traction in the scientific communities. There exist many different techniques to solve the recommendation problem, some of which build upon BP. [citation] recently proposed such a method. 

A run-time efficient implementation of the RS can be of economic relevance for platforms with large and dynamic user-bases. In the following, we present an analysis on how to achieve this objective for the Top-N Recommender System through loopy BP.

\mypar{Related work} 
As mentioned before, Loopy Belief Propagation methods exhibit bad convergence rates. Worse even, convergence often cannot be guaranteed at all. The illustrative work of Elidan et al. \cite{elidan2012residual} proposes a method to improve both convergence rate and, surprisingly, convergence by itself by propagating belief in an informed way through the graph. The technique is called \textit{Residual Belief Propagation} (RBP) for loopy graphs. RBP is applicable for the Recommender System an therefore is 

We know about two popular open-source C++ frameworks for graph-based inference that implement the message passing algorithm \footnote{This term will be used synonymously for BP in this article.} in a generic way: \textit{libDAI} \cite{Mooij_libDAI_10} on the one hand is a clean and accessible piece of software, whereas \textit{OpenGM} \cite{andres2012opengm} on the other hand is written with a higher degree of abstraction. We chose libDAI as our reference, mostly because it offers a native implementation of RBP (contrary to OpenGM).

In the following, we briefly sketch the Top-N Recommender System based on Residual Belief Propagation in section \ref{sec:background} before we extensively inform the reader in section \ref{sec:method} about the optimization techniques that have applied. The results are presented in section \ref{sec:results}.